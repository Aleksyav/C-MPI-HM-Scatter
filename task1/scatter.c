#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <mpi.h>
#include <math.h>

#define DBG 0

int main(int argc, char **argv)
{
    //выберем произвольный размер матрицы в зависимости от числа процессов
    int SIZE = sqrt(atoi(argv[1]));
    //запускаем параллельную секцию для начала моделирования рассылки
    MPI_Init(&argc, &argv);
    int myrank, ranksize;
    MPI_Comm comm;
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
    MPI_Comm_size(MPI_COMM_WORLD, &ranksize);

    //создаем матрицу процессов, тоесть попросим систему исполнения
    //распределить процессы наилучшим способом
    int size[2] = {SIZE, SIZE};
    int periodic[2] = {0};
    MPI_Cart_create(MPI_COMM_WORLD, 2, size, periodic, 0, &comm);

    //зададим карту процессов, элемент карты, это номер процесса, который
    //расположен в данной ячейке транспьютерной матрицы
    int mpi_map[SIZE][SIZE];
    for (int i = 0; i < SIZE * SIZE; i++) {
        int coords[2];
        MPI_Cart_coords(comm, i, 2, coords);
        mpi_map[coords[0]][coords[1]] = i;
    }

    //инициализируем ядро генератора случайных чисел
    srand(ranksize + SIZE);

    //вычислим координату ткущего процесса в транспьютерной матрицы
    int coords[2];
    MPI_Cart_coords(comm, myrank, 2, coords);

    //определим число пересылок до самого дальнего элемента в нашей топологии,
    //это число также равно числу побочных диагоналий в транспьютерной матрице
    int side_diag_count = 2 * SIZE - 1;

    //не будем моделировать пересылку самому себе для процесса инициатора
    if (coords[0] + coords[1] == 0) {
        int t_num = rand() % 1000;

        //проверочный вывод
        printf("Process send %d: [%d][%d], print data: %d\n",
                myrank, 0, 0, t_num);
        fflush(stdout);

        //проверочный вывод
        printf("Process recive %d: [%d][%d], print data: %d\n",
                myrank, 0, 0, t_num);
        fflush(stdout);
    }

    //side_diag_count основных этапов пересылок, идем по побочным диагоналям
    for (int k = side_diag_count - 1; k > 0; k--) {
        // printf("Proc [%d][%d] iter %d\n", coords[0], coords[1], k);
        // fflush(stdout);
        //процесс инициатор рассылки [0, 0]
        if (coords[0] + coords[1] == 0) {
            //начнем рассылку
            //каждое отправляемое число будем генерировать на лету
            //идем по диагоналям матрицы (параметр K определяет этап рассылки и
            //обрабатываемую диагональ)
            for (int i = 0; i < SIZE; i++) {
                for (int j = 0; j < SIZE; j++) {
                    //выбираем элементы матрицы, на которые сейчас будут
                    //отправляться элементы (выбиратеся рабочая диагональ)
                    if (i + j == k) {
                        //сгенерируем отправляемое число
                        int target_num = rand() % 1000;
                        //вычислим адрес следуюего в цепочке процесс
                        int target_proc;
                        if (i == 0) {
                            //идем в процесс сосед справа
                            target_proc = mpi_map[0][1];
                        } else {
                            //идем в процесс сосед снизу
                            target_proc = mpi_map[1][0];
                        }
                        //вычислим тег, в котором закодирован целевой процесс
                        //получатель
                        int target_tag = (i << 16) + j;
                        //создадим переменную для заглушки
                        MPI_Request request;
                        //отправляем данные следующему в цепочке отправок
                        //процессу
                        MPI_Isend(&target_num,  //отправляемое значение
                                  1,            //кол-во значений для отправки
                                  MPI_INT,      //тип отправляемого значения
                                  target_proc,  //номер процесса получателя
                                  target_tag,   //номер целевого процесса получ.
                                  MPI_COMM_WORLD,
                                  &request      //информация об отправке
                                 );

                        //проверочный вывод
                        printf("Process send %d: [%d][%d], print data: %d\n",
                               myrank, i, j, target_num);
                        fflush(stdout);
                    }
                }
            }

            continue;
        }

        //если процесс в столбце с номером 0, то он должен осуществлять цепную
        //двунаправленную передачу, вправо или вниз
        if (coords[1] == 0) {
            //проверим, есть ли процессы из обслуживаемой диагонали в нешей
            //области ответственности
            int count = 0;
            for (int i = coords[0]; i < SIZE; i++) {
                for (int j = coords[1]; j < SIZE; j++) {
                    if (i + j == k) count++;
                }
            }
            //убедимся, что нам нужно что-то делать и получать данные на данном
            //этапе, для последующей их пересылки по транспьютерной матрице

            //в случае если нам не нужно обслуживать процессы, мы не должны
            //ничего делать на этом этапе (не должны принимать данные)
            if (!count) continue;

            //до нас дошла волна пересылок и мы можем продолжить цепочку
            //по пересылке данных по транспьютерной матрице

            //зададим флаг того, что данное предназначалось нам, если мы получили
            //данное которое предназначалось текущему процессу, то данный процесс
            //должен завершить свою работу
            int flag = 0;

            //на данном этапе нам необходимо принять данные для процессов из
            //нашей области ответственности
            //примем все эти данные и перешлем их дальше

            //создадим заявки на получение данных от верхнего процесса
            //для пересылки их процессам, которые находтся в нашей зоне
            //ответственности или перенаправления по транспьютерной матрице
            int recv_buf[count];        //буфер приема данных
            MPI_Request req_buf[count]; //буфер для статуса приема

            //сохраним число процессов, которым нужно отправить данные, и
            //соответственно получить от вышестоящего
            int save_count = count;
            //пройдем по матрице и создадим заявку на получение данных для
            //каждого целевого процесса
            for (int i = coords[0]; i < SIZE; i++) {
                for (int j = coords[1]; j < SIZE; j++) {
                    //к нам должно прийти сообщение для процееса из k-ой диагонали
                    //которая находится под нашей ответственностью
                    if (i + j == k) {

                        //отладочный вывод
                        if (DBG) {
                            printf("Proc [%d][%d] try recv from [%d][%d] to [%d][%d]\n",
                                    coords[0], coords[1], coords[0] - 1, 0, i, j);
                            fflush(stdout);
                        }

                        //определим номер процесса отправителя
                        int source_proc = mpi_map[coords[0] - 1][0];
                        //определим тег сообщения
                        int target_tag = (i << 16) + j;
                        MPI_Irecv(&recv_buf[count - 1],  //буфер приема
                                  1,                     //количество данных
                                  MPI_INT,               //тип переменных
                                  source_proc,           //процесс пред. этапа
                                  target_tag,            //тег целевого процесса
                                  MPI_COMM_WORLD,
                                  &req_buf[count - 1] //статус получения
                                 );
                        //декременируем счетчик процессов, данные от которых
                        //мы уже получили
                        count--;
                    }
                }
            }
            //далее необходимо дождаться всех сообщений и перенаправить их
            //следующим по цепочке процессам
            for (int i = 0; i < save_count; i++) {
                int index;         //индекс сообщения которого мы дождались
                MPI_Status status; //статус с которым мы получили сообщение

                //дождемся произвольного сообщения
                MPI_Waitany(save_count, //количество ожидаемых пересылок
                            req_buf,    //буфер с данными ожидаемых пересылок
                            &index,     //индекс полученной посылки
                            &status     //статус получения посылки
                            );
                //после получения нам необходимо понять куда передавать
                //сообщение дальше
                int target_tag = status.MPI_TAG;
                //вычислим направление по которому отправим данные
                int target_coords[2];
                target_coords[0] = target_tag >> 16;
                target_coords[1] = (target_tag << 16) >> 16;

                //отладочный вывод
                if (DBG) {
                    printf("Proc [%d][%d] true recv from [%d][%d] to [%d][%d]\n",
                            coords[0], coords[1], coords[0] - 1, 0,
                            target_coords[0], target_coords[1]);
                    fflush(stdout);
                }

                //выведем сообщение если данное пренадлежит нам
                if (target_coords[0] == coords[0] &&
                    target_coords[1] == coords[1]) {

                    //проверочный вывод
                    printf("Process recive %d: [%d][%d], print data: %d\n",
                            myrank, coords[0], coords[1], recv_buf[index]);
                    fflush(stdout);

                    //если мы получили наши данные, то надо заканчивать
                    //выполнение пересылок
                    flag = 1;
                    break;
                }
                //перешлем сообщение соседу справа, если целевой процесс
                //находится с нами в одной строчке транспьютерной матрицы
                if (coords[0] == target_coords[0]) {
                    //вычислим номер процесса который находится справа
                    int target_proc = mpi_map[coords[0]][1];
                    //создадим переменную для заглушки
                    MPI_Request request;
                    MPI_Isend(&recv_buf[index],  //отправляемое значение
                              1,                 //кол-во значений для отправки
                              MPI_INT,           //тип отправляемого значения
                              target_proc,       //номер процесса получателя
                              target_tag,        //номер целевого процесса получ.
                              MPI_COMM_WORLD,
                              &request           //информация об отправке
                             );
                    continue;
                }
                //перешлем сообщение соседу снизу, если он находится с нами в
                //столбце
                if (coords[1] <= target_coords[1]) {
                    //вычислим номер процесса который находится снизу
                    int target_proc = mpi_map[coords[0] + 1][0];
                    //создадим переменную для заглушки
                    MPI_Request request;
                    MPI_Isend(&recv_buf[index],  //отправляемое значение
                              1,                 //кол-во значений для отправки
                              MPI_INT,           //тип отправляемого значения
                              target_proc,       //номер процесса получателя
                              target_tag,        //номер целевого процесса получ.
                              MPI_COMM_WORLD,
                              &request           //информация об отправке
                             );
                    continue;
                }
                //мы завершили всевозможные пересылки на данном этапе выполнения
            }
            //завершим пересулку через данный процесс, если мы уже получили
            //данные которые пренадлежали нам
            if (flag) break;
            continue;
        }

        //если процесс в столбцах с номерами больше 0, то он должен осуществлять
        // цепную передачу вправо
        if (coords[1] > 0) {
            //проверим, есть ли процессы из обслуживаемой диагонали в нешей
            //области ответственности линейного процесса
            int flag = 0;
            for (int i = coords[1]; i < SIZE; i++) {
                if (i + coords[0] == k) flag = 1;
            }
            //убедимся, что нам нужно что-то делать и получать данные на данном
            //этапе, для последующей их пересылки по транспьютерной матрице

            //в случае если нам не нужно обслуживать процессы, мы не должны
            //ничего делать на этом этапе (не должны принимать данные)
            if (!flag) continue;

            //до нас дошла волна пересылок и мы можем продолжить цепочку
            //по пересылке данных по транспьютерной матрице

            //на данном этапе нам необходимо принять данные для процессов из
            //нашей области ответственности
            //примем все эти данные и перешлем их дальше

            //создадим заявки на получение данных от верхнего процесса
            //для пересылки их процессам, которые находтся в нашей зоне
            //ответственности или перенаправления по транспьютерной матрице
            int recv;        //буфер приема данных
            MPI_Request req;    //буфер для статуса приема

            //пройдем по матрице и создадим заявку на получение данных для
            //целевого процесса
            for (int i = coords[1]; i < SIZE; i++) {
                //к нам должно прийти сообщение для процееса из k-ой диагонали
                //которая находится под нашей ответственностью
                if (i + coords[0] == k) {

                    //отладочный вывод
                    if (DBG) {
                        printf("Proc [%d][%d] try recv from [%d][%d] to [%d][%d]\n",
                               coords[0], coords[1], coords[0], coords[1] - 1,
                               coords[0], i);
                        fflush(stdout);
                    }

                    //определим номер процесса отправителя
                    int source_proc = mpi_map[coords[0]][coords[1] - 1];
                    //определим тег сообщения
                    int target_tag = (coords[0] << 16) + i;
                    MPI_Irecv(&recv,                 //буфер приема
                              1,                     //количество данных
                              MPI_INT,               //тип переменных
                              source_proc,           //процесс пред. этапа
                              target_tag,            //тег целевого процесса
                              MPI_COMM_WORLD,
                              &req                   //статус получения
                             );

                    //можем получить только одно сообщение
                    break;

                }
            }

            //далее необходимо дождаться прихода сообщения и перенаправить его
            //следующему по цепочке процессу
            int index;         //индекс сообщения которого мы дождались
            MPI_Status status; //статус с которым мы получили сообщение

            //дождемся сообщения
            MPI_Waitany(1,          //количество ожидаемых пересылок
                        &req,       //буфер с данными ожидаемых пересылок
                        &index,     //индекс полученной посылки
                        &status     //статус получения посылки
                        );
            //после получения нам необходимо понять куда передавать
            //сообщение дальше
            int target_tag = status.MPI_TAG;
            //вычислим направление по которому отправим данные
            int target_coords[2];
            target_coords[0] = target_tag >> 16;
            target_coords[1] = (target_tag << 16) >> 16;

            //отладочный вывод
            if (DBG) {
                printf("Proc [%d][%d] true recv from [%d][%d] to [%d][%d]\n",
                        coords[0], coords[1], coords[0], coords[1] - 1,
                        target_coords[0], target_coords[1]);
                fflush(stdout);
            }

            //выведем сообщение если данное пренадлежит нам
            if (target_coords[0] == coords[0] &&
                target_coords[1] == coords[1]) {

                //проверочный вывод
                printf("Process recive %d: [%d][%d], print data: %d\n",
                        myrank, coords[0], coords[1], recv);
                fflush(stdout);

                //если мы получили свое данное, то процесс больше не должен
                //продолжать выполняться
                break;
            }
            //перешлем сообщение соседу справа
            //вычислим номер процесса который находится справа
            int target_proc = mpi_map[coords[0]][coords[1] + 1];
            //создадим переменную для заглушки
            MPI_Request request;
            //переотправим сообщение
            MPI_Isend(&recv,             //отправляемое значение
                      1,                 //кол-во значений для отправки
                      MPI_INT,           //тип отправляемого значения
                      target_proc,       //номер процесса получателя
                      target_tag,        //номер целевого процесса получ.
                      MPI_COMM_WORLD,
                      &request           //информация об отправке
                     );

            continue;
        }
    }

    MPI_Finalize();
    return 0;
}
